# UNTR
The code will be given here soon after the paper is accepted.

Blind Hyperspectral Unmixing (HU) aims to obtain subpixel information by simultaneously estimating endmembers and their corresponding abundances from observed data. Contemporary data-driven blind HU techniques face two main challenges. First, most deep learning-based methods are developed with the Linear Mixing Model (LMM) assumption, resulting in limited exploration of nonlinear mixing approaches. Second, these methods heavily rely on proper initialization of endmembers, and their performance can significantly degrade without suitable initialization methods like VCA. This article introduces a novel unmixing paradigm called Unmixing Transformer (UNTR), which leverages the attention mechanism of Transformer to address the aforementioned challenges. UNTR presents a novel decoding architecture to generate the reconstruction embedding queries using estimated abundances and endmember embedding queries. By incorporating the mixing mechanism into the decoder, UNTR effectively reproduces the distribution properties of endmembers in the observed scene without relying on specific endmember initialization. Experimental results obtained from diverse hyperspectral unmixing datasets demonstrate that UNTR, free from the constraint of endmember initialization, achieves exceptional accuracy in both abundance and endmember estimation, outperforming a variety of competing algorithms.
